{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cda661-535e-4d34-a92a-c6234f2c7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlled_generation(prompt, threshold=0.7, max_len=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_len+len(input_ids[0]),\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        top_p=0.9,\n",
    "        temperature=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    prompt_embedding = model.transformer.wte(input_ids).mean(dim=1)\n",
    "\n",
    "    for i in range(len(generated_text)):\n",
    "        next_token = tokenizer.encode(generated_text[i], return_tensors='pt')\n",
    "        next_token_embedding = model.transformer.wte(next_token).mean(dim=1)\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "\n",
    "        if cosine_sim < threshold:\n",
    "            continue\n",
    "        else:\n",
    "            while cosine_sim >= threshold and len(generated_text) < max_len:\n",
    "                next_token = model.generate(\n",
    "                    input_ids=output[:, :-1],\n",
    "                    max_length=output.shape[-1] + 1,\n",
    "                    do_sample=True,\n",
    "                    top_k=0,\n",
    "                    top_p=0.9,\n",
    "                    temperature=1,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )[:, -1]\n",
    "                next_token_embedding = model.transformer.wte(next_token).mean(dim=1)\n",
    "                cosine_sim = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "\n",
    "                output = torch.cat([output, next_token.unsqueeze(0)], dim=-1)\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e785985-94dc-489b-b2f4-91e20b99deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telebot import TeleBot\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from transformers import HfArgumentParser\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from utils.gen_answer_persona import GenAnswerPersona\n",
    "from utils.arguments_inference import InteractionArguments\n",
    "\n",
    "\n",
    "# Should have no parent classes with fields without defaults (if one big `args` needed)\n",
    "@dataclass\n",
    "class TelegaArguments(InteractionArguments):\n",
    "    bot_token: str = field(default=\"\", metadata={\"help\": \"Bot token.\"})\n",
    "\n",
    "# Описываем парсер аргументов командной строки\n",
    "parser = HfArgumentParser(TelegaArguments)\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))[0]\n",
    "else:\n",
    "    args = parser.parse_args_into_dataclasses()[0]\n",
    "\n",
    "if args.tokenizer_name is None:\n",
    "    args.tokenizer_name = args.model_checkpoint\n",
    "\n",
    "log_file = \"log_file_\" + args.bot_token[-5:] + \".log\"\n",
    "log_fail_file = \"log_fail_file.log\"\n",
    "\n",
    "bot = TeleBot(args.bot_token, threaded=False)\n",
    "\n",
    "Chat = GenAnswerPersona(args=args)\n",
    "\n",
    "act_rep = defaultdict(list)\n",
    "win = args.max_history\n",
    "\n",
    "\n",
    "# Обработчик команд\n",
    "@bot.message_handler(commands=[\"len_context\"])\n",
    "def handle_intent_list(message):\n",
    "    global win\n",
    "    w = len(Chat.history)\n",
    "    st = f\"Длина контекста: {w}\"\n",
    "    bot.send_message(message.chat.id, st)\n",
    "\n",
    "\n",
    "@bot.message_handler(commands=[\"start\"])\n",
    "def handle_intent_list(message):\n",
    "    st = \"Привет. Давай поболтаем о чем-нибудь\"\n",
    "    bot.send_message(message.chat.id, st)\n",
    "    \n",
    "\n",
    "@bot.message_handler(commands=[\"del_context\"])\n",
    "def handle_intent_list(message):\n",
    "    st = \"Контекст удален\"\n",
    "    bot.send_message(message.chat.id, st)\n",
    "    global act_rep\n",
    "    act_rep[message.chat.id] = []\n",
    "\n",
    "\n",
    "@bot.message_handler(commands=[\"context\"])\n",
    "def handle_intent_list(message):\n",
    "    global act_rep\n",
    "    bot.send_message(message.chat.id, \"Context: \" + \" || \".join(act_rep[message.chat.id]))\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=[\"text\"])\n",
    "def handle_text(message):\n",
    "    global act_rep\n",
    "    context = message.text\n",
    "    act_rep[message.chat.id].append(context)\n",
    "    chat_id_history = act_rep[message.chat.id]\n",
    "\n",
    "    bot.send_chat_action(message.chat.id, \"typing\")\n",
    "\n",
    "    # answers, add_info = Chat.get_reply_batched(input_data, persona_code=args.persona_code)\n",
    "    # answer = answers[0]\n",
    "    \n",
    "\n",
    "    act_rep[message.chat.id].append(answer)\n",
    "\n",
    "    with codecs.open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            str(datetime.date.today().strftime(\"%Y-%m-%d\"))\n",
    "            + \";\"\n",
    "            + str(message.from_user.id)\n",
    "            + \";\"\n",
    "            + str(message.chat.id)\n",
    "            + \";\"\n",
    "            + str(message.message_id)\n",
    "            + \";\"\n",
    "            + context\n",
    "            + \";\"\n",
    "            + message.text\n",
    "            + \";\"\n",
    "            + answer\n",
    "            + \"\\n\",\n",
    "        )\n",
    "\n",
    "    bot.send_message(\n",
    "        message.chat.id,\n",
    "        text=answer,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VMAX = 0\n",
    "    while VMAX <= 100:\n",
    "        try:\n",
    "            bot.polling(none_stop=True, interval=0, timeout=2000)\n",
    "        except:\n",
    "            VMAX += 1\n",
    "            print(\"Error\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
