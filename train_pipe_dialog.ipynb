{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5434c54-063d-42ca-b55e-770748c56bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "Cloning into 'instruct_rugptlarge'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 89 (delta 42), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (89/89), 1.43 MiB | 3.29 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 4.43 GiB | 86.11 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/AlexWortega/instruct_rugptlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453186ed-5c64-497b-ad55-b33426939cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting dask\n",
      "  Downloading dask-2023.4.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 9.5 MB/s eta 0:00:01\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "\u001b[?25hCollecting partd>=1.2.0\n",
      "  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (2022.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (6.0.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (2.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (8.1.3)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alexw/.local/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask) (3.15.0)\n",
      "Installing collected packages: locket, partd, dask\n",
      "Successfully installed dask-2023.4.0 locket-1.0.0 partd-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e64c48-160f-4f76-a970-72a1e763a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import wandb\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a386dbb0-ac53-4425-8b26-13bdada89ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 12 21:51:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    55W / 300W |  25423MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    44W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3653010      C   /usr/bin/python3                20963MiB |\n",
      "|    0   N/A  N/A   3733118      C   /usr/bin/python3                 4457MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d59ac5-d63b-4a35-947d-8ebdaaf4bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9abd70-d560-4033-915f-688c6515d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68aac0b0-5db8-4324-9ab7-9f5797ad83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load(\"config.yaml\") #get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed32691-fd69-4c46-acf7-507a3520bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(conf.model.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c311d91a-ca8e-4405-adfc-c358c8f83e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['user:', 'bot:']}\n",
    "tokenizer.add_special_tokens({'eos_token': '<instructionE>'})\n",
    "tokenizer.add_special_tokens({'bos_token': '<instructionE>'})\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model = GPT2LMHeadModel.from_pretrained(conf.model.model_name_or_path)\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed97099-b504-4338-b42d-5b7540b4a32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5eebf18-57ca-4726-a442-42056d6bb292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: ru_turbo_saiga/default\n",
      "Found cached dataset ru_turbo_saiga (/mnt/opt/alexw/Kar/chatbot_saiga_dataset/./IlyaGusev___ru_turbo_saiga/default/0.0.1/28415c7ce4ae7d7de9064f6f7e18a089ba79edce3adc4a43fdbb09e9cd39f16c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f678e70342f46b4b4c349e80cca8dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_helper = load_dataset(conf.args.dataset_name, cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26db0d06-ba07-4b57-9bde-51628d923f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration zjkarina--matreshka-8e0974f9d53dcb5f\n",
      "Found cached dataset parquet (/mnt/opt/alexw/Kar/chatbot_saiga_dataset/./zjkarina___parquet/zjkarina--matreshka-8e0974f9d53dcb5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3195188aa0534fb98ca0623b47637208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_chat = load_dataset(\"zjkarina/matreshka\", cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b450574-6100-49da-8b60-ad92111b6e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['role', 'dialog'],\n",
       "        num_rows: 1736\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['role', 'dialog'],\n",
       "        num_rows: 6943\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707c2762-0370-45f4-875b-8475363f77f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289097cc-3552-4765-b6c2-12dad4cda552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41dfeea2-3b09-459e-8f89-6076841ab158",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sen in enumerate(zip(dataset_chat['train'][2]['role'], dataset_chat['train'][2]['dialog'])):\n",
    "    if sen[0] == 'user' and idx != 0:\n",
    "        sen = (f\" <instructionE> user\", sen[1])\n",
    "        dialog += ': '.join(sen)\n",
    "    elif sen[0] == 'user' and idx == 0:\n",
    "        sen = (f\"user\", sen[1])\n",
    "        dialog += ': '.join(sen)\n",
    "    else:\n",
    "        sen = (f\" bot\", sen[1])\n",
    "        dialog += ': <instructionE> '.join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f249f905-6e18-45ec-ac8c-6f5a9f5a3b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dia \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset_saiga):\n\u001b[1;32m      3\u001b[0m     dialog \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, sen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdia\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, dia[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialog\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sen[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m             sen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <instructionE> user\u001b[39m\u001b[38;5;124m\"\u001b[39m, sen[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "dataset_saiga = dataset_chat['train'][:5]\n",
    "for dia in tqdm(dataset_saiga):\n",
    "    dialog = \"\"\n",
    "    for idx, sen in enumerate(zip(dia['role'], dia['dialog'])):\n",
    "        if sen[0] == 'user' and idx != 0:\n",
    "            sen = (f\" <instructionE> user\", sen[1])\n",
    "            dialog += ': '.join(sen)\n",
    "        elif sen[0] == 'user' and idx == 0:\n",
    "            sen = (f\"user\", sen[1])\n",
    "            dialog += ': '.join(sen)\n",
    "        else:\n",
    "            sen = (f\" bot\", sen[1])\n",
    "            dialog += ': <instructionE> '.join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b34b6825-475d-46d9-9b9d-fd755655b0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user: Как твое настроение сегодня? bot: <instructionE> Привет! Как дела? bot: <instructionE> Мое настроение сегодня нормальное, а у тебя? <instructionE> user: Я думаю, это будет хороший день. А как твои дела?user: Привет! Как дела? bot: <instructionE> Привет! У меня все хорошо, а у тебя? <instructionE> user: У меня тоже хорошо. Знаешь ли ты, что самая высокая гора в мире - это Эверест? bot: <instructionE> Да, я знаю. А ты знаешь, что пингвины могут плавать со скоростью до 22 миль в час? <instructionE> user: Нет, не знал. Интересный факт. А знаешь ли ты, что некоторые киты могут жить до 200 лет? bot: <instructionE> Да, я знаю. Это действительно удивительно. А ты знаешь, что наша галактика Млечный Путь содержит более 200 миллиардов звезд? <instructionE> user: Да, я знаю об этом. Интересно, на сколько еще фактов ты способен? bot: <instructionE> Меня программировали умными людьми, поэтому я знаю много фактов. Например, пчела может облететь до 90 тысяч цветов за один день. <instructionE> user: Невероятно! Я не знал об этом. Спасибо за интересный факт. Пока! bot: <instructionE> До свидания!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45b942c1-d413-4b65-93b6-f5769ec6bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset_helper, dataset_chat):\n",
    "        self.tokenized = []\n",
    "        dataset_saiga = dataset_helper['train']['messages']\n",
    "        dataset_my_train = dataset_chat['train']\n",
    "        dataset_my_val = dataset_chat['validation']\n",
    "        \n",
    "        for dia in tqdm(dataset_saiga):\n",
    "            dialog = \"\"\n",
    "            for idx, sen in enumerate(zip(dia['role'], dia['content'])):\n",
    "                if sen[0] == 'user' and idx != 0:\n",
    "                    sen = (f\" <instructionE> user\", sen[1])\n",
    "                    dialog += ': '.join(sen)\n",
    "                elif sen[0] == 'user' and idx == 0:\n",
    "                    sen = (f\"user\", sen[1])\n",
    "                    dialog += ': '.join(sen)\n",
    "                else:\n",
    "                    sen = (f\" bot\", sen[1])\n",
    "                    dialog += ': <instructionE> '.join(sen)\n",
    "            enc = self._encode(text=dialog, tokenizer=tokenizer)\n",
    "            self.tokenized += [enc]\n",
    "        \n",
    "        \n",
    "        # for input_tokens, output_tokens in samples:\n",
    "        #     sen = f\"{tokenizer.bos_token} bot: {input_tokens} user: {output_tokens} {tokenizer.eos_token}\"\n",
    "        #     enc = self._encode(text=sen, tokenizer=tokenizer)\n",
    "        #     self.tokenized += [enc]\n",
    "        for dia in tqdm(dataset_my_train):\n",
    "            dialog = \"\"\n",
    "            try:\n",
    "                for idx, sen in enumerate(zip(dia['role'], dia['dialog'])):\n",
    "                    if sen[0] == 'user' and idx != 0:\n",
    "                        sen = (f\" <instructionE> user\", sen[1])\n",
    "                        dialog += ': '.join(sen)\n",
    "                    elif sen[0] == 'user' and idx == 0:\n",
    "                        sen = (f\"user\", sen[1])\n",
    "                        dialog += ': '.join(sen)\n",
    "                    else:\n",
    "                        sen = (f\" bot\", sen[1])\n",
    "                        dialog += ': <instructionE> '.join(sen)\n",
    "                enc = self._encode(text=dialog, tokenizer=tokenizer)\n",
    "                self.tokenized += [enc]\n",
    "            except:\n",
    "                pass\n",
    "        for dia in tqdm(dataset_my_val):\n",
    "            dialog = \"\"\n",
    "            try:\n",
    "                for idx, sen in enumerate(zip(dia['role'], dia['dialog'])):\n",
    "                    if sen[0] == 'user' and idx != 0:\n",
    "                        sen = (f\" <instructionE> user\", sen[1])\n",
    "                        dialog += ': '.join(sen)\n",
    "                    elif sen[0] == 'user' and idx == 0:\n",
    "                        sen = (f\"user\", sen[1])\n",
    "                        dialog += ': '.join(sen)\n",
    "                    else:\n",
    "                        sen = (f\" bot\", sen[1])\n",
    "                        dialog += ': <instructionE> '.join(sen)\n",
    "                enc = self._encode(text=dialog, tokenizer=tokenizer)\n",
    "                self.tokenized += [enc]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.tokenized[item] \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def regular(string, tokenizer):\n",
    "#         new_string = \"\"\n",
    "#         slicer = 0\n",
    "#         counter = 0\n",
    "#         for match in re.finditer(r'\\b\\w+:\\s', string):\n",
    "#             new_string += string[slicer:match.start()]# Append the text before the match\n",
    "#             counter += 1\n",
    "#             if counter % 2 == 1 and ':' not in new_string[-3:]: # Check if the match is in an even-numbered position\n",
    "#                 new_string += f\" {tokenizer.bos_token} user: \"\n",
    "#             elif counter % 2 == 0 and ':' not in new_string[-3:]:\n",
    "#                 new_string += \" bot: \"\n",
    "#             slicer = match.end() # Update the count to the end of the match\n",
    "\n",
    "#         new_string += string[slicer:] # Append any remaining text after the last match\n",
    "#         new_string = re.sub(r'[\\r\\n]+', '', new_string) # Remove any remaining line breaks\n",
    "#         return new_string\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode(text, tokenizer):\n",
    "        encoded_sample = tokenizer(text, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b739053b-3226-4862-a299-b1a3effadb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36352/36352 [00:45<00:00, 793.15it/s]\n",
      "100%|██████████| 6655/6655 [00:05<00:00, 1317.81it/s]\n",
      "100%|██████████| 1664/1664 [00:01<00:00, 1325.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data = DialogDataset(tokenizer=tokenizer, dataset_helper=dataset_helper, dataset_chat=dataset_chat)\n",
    "train_dataloader = DataLoader(\n",
    "        data, batch_size=conf.args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f94564d-4c5d-4c6f-8903-27fd18ef1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarina_romanova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/opt/alexw/Kar/chatbot_saiga_dataset/wandb/run-20230513_231255-u8whskhn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karina_romanova/diplom/runs/u8whskhn' target=\"_blank\">frombase_5epoch_saiga+my</a></strong> to <a href='https://wandb.ai/karina_romanova/diplom' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karina_romanova/diplom' target=\"_blank\">https://wandb.ai/karina_romanova/diplom</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karina_romanova/diplom/runs/u8whskhn' target=\"_blank\">https://wandb.ai/karina_romanova/diplom/runs/u8whskhn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/karina_romanova/diplom/runs/u8whskhn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8a1d291be0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='diplom', name='frombase_5epoch_saiga+my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1af581b6-423f-4556-ae9e-edca26563248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, decay):\n",
    "        super(EMA, self).__init__()\n",
    "        self.decay = decay\n",
    "        self.shadow_params = {}\n",
    "\n",
    "    def forward(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if name not in self.shadow_params:\n",
    "                    self.shadow_params[name] = param.data.clone()\n",
    "                else:\n",
    "                    self.shadow_params[name] -= (1 - self.decay) * (self.shadow_params[name] - param.data)\n",
    "                param.data = self.shadow_params[name]\n",
    "                \n",
    "ema = EMA(decay=0.992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb35990a-36a2-4a07-b2c7-ea454b4ea12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from typing import Callable, Iterable, Tuple\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import math\n",
    "\n",
    "class ChildTuningAdamW(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[torch.nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "        reserve_p = 1.0,\n",
    "        mode = None\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        self.gradient_mask = None\n",
    "        self.reserve_p = reserve_p\n",
    "        self.mode = mode\n",
    "\n",
    "    def set_gradient_mask(self, gradient_mask):\n",
    "        self.gradient_mask = gradient_mask\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (:obj:`Callable`, `optional`): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                # =================== HACK BEGIN =======================         \n",
    "                if self.mode is not None:\n",
    "                    if self.mode == 'ChildTuning-D':\n",
    "                        if p in self.gradient_mask:\n",
    "                            grad *= self.gradient_mask[p]\n",
    "                    else: \n",
    "                        # ChildTuning-F\n",
    "                        grad_mask = Bernoulli(grad.new_full(size=grad.size(), fill_value=self.reserve_p))\n",
    "                        grad *= grad_mask.sample() / self.reserve_p\n",
    "                # =================== HACK END =======================\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                p.data.add_(p.data, alpha=-group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d24b9-3653-47a6-b537-6f109f28d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexw/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 30%|██▉       | 4442/14890 [58:35<2:18:07,  1.26it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 84%|████████▍ | 12565/14890 [2:45:45<30:38,  1.26it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 39%|███▉      | 5825/14890 [1:16:47<1:59:31,  1.26it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 79%|███████▉  | 11799/14890 [2:35:35<40:46,  1.26it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 36%|███▌      | 5357/14890 [1:10:37<2:05:41,  1.26it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 92%|█████████▏| 13742/14890 [3:01:10<15:07,  1.27it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 14890/14890 [3:16:18<00:00,  1.26it/s]\n",
      " 26%|██▌       | 3835/14890 [50:33<2:26:02,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import torch_optimizer\n",
    "\n",
    "ema = EMA(decay=0.992)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, betas=(0.95, 0.99), weight_decay=0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader)\n",
    "            )\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "    \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        # Переносим тензоры на устройство (GPU)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = input_ids\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)[0]\n",
    "        wandb.log({\"loss\":  loss})\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        ema(model)\n",
    "        scheduler.step()\n",
    "    model.save_pretrained(f'boseos_saiga+my_{epoch+1}')\n",
    "model.eval()\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e244458-cc5c-4466-9475-f8f4383436dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " wandb.log({\"lossq\":  loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55dd8cc6-6f4f-4fb1-99de-a3588f35d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c972a3f-57a4-49eb-8ee0-f99d688bff09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad40692a-7380-4ba0-8808-673d8445f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('checkpoint_alldia_in')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
