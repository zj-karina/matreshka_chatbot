{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71796d1b-c7b3-4e4f-959c-c83f4a7cd1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 12:30:37.543206: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-15 12:30:37.593280: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-15 12:30:37.595297: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fca09f-fa72-426e-b3ac-15d0ae06c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cb8729-6df1-431f-8eaa-02fe993871aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4f5446-3aed-46d2-b35e-2f4933b86c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load(\"config.yaml\") #get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5981f496-f185-4aa4-8ed4-0fa7ced51986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 1536)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(conf.model.model_name_or_path)\n",
    "special_tokens_dict = {'additional_special_tokens': ['user:', 'bot:']}\n",
    "tokenizer.add_special_tokens({'eos_token': '<instructionE>'})\n",
    "tokenizer.add_special_tokens({'bos_token': '<instructionE>'})\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model = GPT2LMHeadModel.from_pretrained('/mnt/opt/alexw/Kar/chatbot_saiga_dataset/boseos_saiga+my_2')\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22505742-a70e-4aa6-ac3c-5f6c87615f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5205b958-a35f-4174-aecf-33ed39336d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([50264]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ba879b-5369-4330-9d0c-423c3900ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "383c9428-ef6c-4cb8-a538-7c0275d31a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50261\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "642112ee-8529-4b1e-b846-31ab160fcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlled_generation(prompt, word, threshold=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        min_length=20,\n",
    "        max_new_tokens=520,\n",
    "        do_sample=True,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        top_k=70,\n",
    "        top_p=0.6,\n",
    "        temperature=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        bad_words_ids=[[225], [50263], [50264], [50265], [203]],\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(tokenizer.decode(output[0]))\n",
    "    # generated_text = tokenizer.decode(output[0])\n",
    "    \n",
    "    input_ids_word = tokenizer.encode(word, return_tensors='pt').to(device)\n",
    "    input_word = tokenizer.encode(tokenizer.eos_token + ' ' + word, return_tensors='pt').to(device)\n",
    "    prompt_embedding = model.transformer.wte(input_ids_word).mean(dim=1)\n",
    "    output_all = torch.cat([input_word, output], dim=-1)\n",
    "    # output_all = output\n",
    "    cosine_sim_after = -1\n",
    "    for i in range(len(output_all[0])):\n",
    "        next_token = output_all[:, i]\n",
    "        next_token_embedding = model.transformer.wte(next_token).mean(dim=1)\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "        \n",
    "        \n",
    "        if cosine_sim.item() < threshold and output_all[0, i] == 50261 and i > (len(input_ids[0])+ len(input_ids_word[0])):\n",
    "            while cosine_sim_after <= cosine_sim:\n",
    "            # print(output[:, :i])\n",
    "                print('before: ', cosine_sim.item())\n",
    "                next_new_token = model.generate(\n",
    "                    input_ids=output_all[:, :i],\n",
    "                    bos_token_id=tokenizer.bos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    do_sample=True,\n",
    "                    max_new_tokens=150,\n",
    "                    top_k=70,\n",
    "                    top_p=0.6,\n",
    "                    temperature=1.2,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    bad_words_ids=[[225], [50263], [50264], [50265]],\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "            # print(next_token)\n",
    "                next_token_embedding = model.transformer.wte(next_new_token).mean(dim=1)\n",
    "                cosine_sim_after = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "                print(tokenizer.decode(next_new_token[0]))\n",
    "                print('after: ', cosine_sim_after.item())\n",
    "            \n",
    "            output_all = next_new_token\n",
    "            # output[0, i] = next_token\n",
    "        elif cosine_sim.item() < threshold and i > (len(input_ids[0])+ len(input_ids_word[0])):#(len(input_ids[0])+ len(input_ids_word[0]))\n",
    "            # print(abs(cosine_sim.item()))\n",
    "            while cosine_sim_after <= cosine_sim:\n",
    "                print('output ', tokenizer.decode(output_all[0, :i]))\n",
    "                print('before: ', cosine_sim.item())\n",
    "                next_new_token = model.generate(\n",
    "                    input_ids=output_all[:, :i],\n",
    "                    max_length=output_all.shape[-1] + 1,\n",
    "                    bos_token_id=tokenizer.bos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    do_sample=True,\n",
    "                    min_length=1,\n",
    "                    top_k=100,\n",
    "                    top_p=0.2,\n",
    "                    temperature=1.2,\n",
    "                    no_repeat_ngram_size=1,\n",
    "                    bad_words_ids=[[225], [50263], [50264], [50265]],\n",
    "                    num_return_sequences=1\n",
    "                )[:, i]\n",
    "            # print(next_token)\n",
    "                next_token_embedding = model.transformer.wte(next_new_token).mean(dim=1)\n",
    "                cosine_sim_after = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "                print(tokenizer.decode(next_new_token[0]))\n",
    "                print('after: ', cosine_sim_after.item())\n",
    "                # output = torch.cat([output, next_token.unsqueeze(0)], dim=-1)\n",
    "            output_all[0, i] = next_new_token\n",
    "            \n",
    "                \n",
    "        \n",
    "    generated_text = tokenizer.decode(output_all[0])\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9d9b541a-8e73-4a5a-ba1b-3c0ab26bb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlled_generation(prompt, word):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        min_length=20,\n",
    "        max_new_tokens=520,\n",
    "        do_sample=True,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        top_k=70,\n",
    "        top_p=0.6,\n",
    "        temperature=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        bad_words_ids=[[225], [50263], [50264], [50265], [203]],\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(tokenizer.decode(output[0]))\n",
    "    \n",
    "    input_ids_word = tokenizer.encode(word, return_tensors='pt').to(device)\n",
    "    prompt_embedding = model.transformer.wte(input_ids_word).mean(dim=1)\n",
    "    \n",
    "    input_word = tokenizer.encode(tokenizer.eos_token + ' ' + word, return_tensors='pt').to(device)\n",
    "    # input_all = torch.cat([input_word, input_ids], dim=-1)\n",
    "    input_all = input_ids\n",
    "    next_token_embedding = model.transformer.wte(output).mean(dim=1)\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "    cosine_sim_after = cosine_sim\n",
    "    bad_output = torch.cat([input_ids, torch.tensor([[tokenizer.eos_token_id]]).to(device)], dim=-1)\n",
    "    next_new_token = bad_output\n",
    "    \n",
    "    if cosine_sim.item() < 0:   \n",
    "        while cosine_sim_after.item() <= cosine_sim.item() or tokenizer.decode(next_new_token[0]) == tokenizer.decode(bad_output[0]):\n",
    "            next_new_token = model.generate(\n",
    "                input_ids=input_all,\n",
    "                bos_token_id=tokenizer.bos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=150,\n",
    "                top_k=100,\n",
    "                top_p=0.2,\n",
    "                temperature=1.2,\n",
    "                no_repeat_ngram_size=2,\n",
    "                bad_words_ids=[[225], [50263], [50264], [50265]],\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            next_token_embedding = model.transformer.wte(next_new_token).mean(dim=1)\n",
    "            cosine_sim_after = torch.nn.functional.cosine_similarity(prompt_embedding, next_token_embedding, dim=-1)\n",
    "            \n",
    "    output_all = next_new_token   \n",
    "                    \n",
    "    generated_text = tokenizer.decode(output_all[0])\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8876653d-3295-4358-b497-9a3605702b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<instructionE> user: что ты делаешь? bot: <instructionE>Я читаю книги и смотрю фильмы. А ты?<instructionE>\n",
      "<instructionE> user: что ты делаешь? bot: <instructionE>Я занимаюсь разработкой программного обеспечения.<instructionE>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"<instructionE> user: что ты делаешь? bot: <instructionE>\"\n",
    "generated_text = controlled_generation(prompt, word=\"ты ребенок\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76e21441-c171-4541-a154-75a496feef8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  как вывести график на python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user: как вывести график на python? bot: <instructionE> `your_image()` позволяет выводить координаты в графике на экране. Чтобы вывести этот график, нужно использовать функцию set(). Она возвращает значение параметра функции, которая принимает аргумент. Например: \n",
      "``pythоны = {'name', 'John', 30};`. Здесь мы передаем функцию и выводим ее на экран с помощью функции get(), что означает \"John\". Если вы хотите получить координаты от 1 до 5 точек, можно передать их в функцию\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>>> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inp \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1182\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1225\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_k\": 70,\n",
    "        \"top_p\": 0.8,\n",
    "        \"do_sample\": True,  \n",
    "        # \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"bos_token_id\": tokenizer.bos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"temperature\": 0.8,\n",
    "        \"use_cache\": True,\n",
    "        \"repetition_penalty\": 1.2,  \n",
    "        \"bad_words_ids\": [[225], [50263], [50264], [50265], [203]],\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "min_length=20\n",
    "context = ''\n",
    "inp = ''\n",
    "while True:\n",
    "    inp = input(\">>> \")\n",
    "    if inp == \"stop\":\n",
    "        break\n",
    "    inp_prepared = context + ' user: ' + inp + ' bot: <instructionE> '\n",
    "    prepared = tokenizer.encode(inp_prepared, return_tensors='pt').to(device)\n",
    "    out = model.generate(min_length=int(min_length+len(context)*0.5), input_ids=prepared, **gen_kwargs)\n",
    "    generated = tokenizer.decode(out[0])\n",
    "    print(generated)\n",
    "    # print(gn_result)\n",
    "    \n",
    "    context = generated\n",
    "    # if min_length > 480:\n",
    "    # print(context.split('user:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "829b5ae2-be6a-40b8-82bb-e20c1b658312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50261,   225, 50264,   417, 41213,  1642,   282,   515,  6784,   225,\n",
       "        50265,   225,     5,  9230,   329,  1304,  4162,    35,   889,   895,\n",
       "          953,  9245,   281, 30924, 18183,   289, 44437,  1163,   979, 50261],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6444c81-e213-4c0d-9983-5d0ca0f3f5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<instructionE>'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8557a98-308c-4864-9295-3be8cf608d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
